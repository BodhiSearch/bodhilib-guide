bodhilib is a python library, that allows you to use various LLM APIs using a consistent interface. 
The main advantages of the API are -
- Open source
- Plugin Architecture
- Simple composable design
- Pythonic interfaces
- Re-use and not re-inventing the wheel
- No walled gardens, plus offering, or preferred partner implementation in the core


Few of the benefits of Plugin Architectures are -
- Core repo only contains core interface, very light weight and slim
- Easy to understand decluttered core interface
- All funcitonalities through plugins
- Even the features provided by bodhilib packaged as `bodhiext.*` package
- Easy extensibility through plugins
- Extensibility for your particular use case
- Community and distributed development possible through plugin architecture
- No single-repo PR hell
- No single-repo bug fix dependency
- No preferred partner only implementation
- Product companies can own and release their own plugins
- Plugin marketplace to search for implementations
- Plugin extensibility allows for Rust based implementation for performance critical interface
- No re-inventing the wheel, re-use existing implementations, e.g. monitoring through existing stack
- Using python namespace packages for `bodhiext.*` plugins for easy recall of implementations


Few of the benefits of Composable design -
- Design inspired by Haskell, and other functional languages
- Composable design
- Parametric polymorphism, methods can take multiple type of arguments, handles implementation internally

Few of the benefits of Pythonic interfaces -
- Pythonic interfaces, not Java OO hell design

The main models of the libraries are -
- Role
  Can be system, ai, or user. indicates the role of the prompt.
- Source
  Can be input, output. indicates if the prompt is input, or generated as output from LLM.
- Prompt
  The main model used to send messages to LLM, and receive generated response. text = the prompt text to send, role = the role of the prompt, source = input or output indicating the direction of message
- PromptStream
  The prompt as a streaming async response rather than sync response. So you receive the response as a stream of messages, rather than a single response. Iterate over the stream to get the messages as they are generated. Pythonic interface of a Generator/Iterator pattern.
- TemplateFormat - indicates the format of template, can be fstring, jinja2, bodhilib-fstring, bodhilib-jinja2.
    fstring - injects the variable using python f-string
    jinja2 - uses jinja2 template to inject the variables, used for more complex prompt involving looping and if-else conditions.
    bodhilib-fstring - parses the serialized prompt template and then uses f-string to inject variables. 
    bodhilib-jinja2 - parses the serialized prompt template and then uses jinja2 template to inject variables.
- PromptTemplate
  reads/writes parameterized prompt templates to file. Allows reuse, sharing of parameterized prompts templates.
- Document
  Represents a resource to process. Contains text as the main content, and metadata as key/value argument.
- Node
  Represents a processible chunk split from Document. Contains text as the main content, and metadata as key/value argument. Also contains reference to the parent Document as parent. Contains id and Embedding to be stored in Vector DB.
- Emebdding
  To represent vector representation of a text/Node chunk.
- SupportsText
  Protocol to indicate if a class has property text. Majority of the models implement this protocol, like Prompt, Document, Node, etc.
- PathLike
  Flexible input to take a local file location, as str or as python Path
- TextLike
  Flexible type to represent input as str or as SupportsText
- SerializedInput
  Flexible type to represent input as str or as SupportsText or as serialized object as dict

The main components of the library are -
- PromptSource
  The interface to browse and search through prompt templates.
- DataLoader
  The interface to load the data from various sources as Document, e.g. file, database, etc.
- Splitter
  The interface to split the Document into processible entities as Node.
- Embedder
  The interface to embed the Node chunk into embedding.
- VectorDB
  The interface to store, retrieve and search the vector representation of the Node chunk.
- LLM
  To interact with LLM API, and generate response for a prompt.

===
Following is the Retrieval Augmented Generation (RAG) workflow using bodhilib components.

::: {.cell .markdown}
# Extractive QnA

So far, we have seen how to load our documents, split them into Nodes,
create Embeddings for the Nodes, insert them into Vector database, and
then finally query them for a given input query.

In this Getting Started guide, we are going to see how to use LLM to do
an extractive QnA on the returned nodes, and display result directly. We
are going to use service and components that we have already covered in
the guide to build this workflow.

Let\'s get started.
:::

::: {.cell .markdown}
## Setup

1.  Ensure bodhilib is installed.
2.  Ensure LLM extension `bodhiext.openai` is installed.
3.  Ensure Embedder extension `bodhiext.sentence_transformers` is
    installed.
4.  Ensure VectorDB extension `bodhiext.qdrant` is installed.
:::

::: {.cell .code execution_count="1"}
``` python
!pip install -q bodhilib bodhiext.openai bodhiext.sentence_transformers bodhiext.qdrant python-dotenv
```
:::

::: {.cell .code execution_count="2"}
``` python
# prepare the node embeddings for the Paul Graham's essay:
# 1. Load the Paul Graham essays from data/data-loader directory using `file` DataLoader
# 2. Convert it into Nodes using sentence_splitter
# 3. Enrich node embeddings using the sentence_transformers
import os
from pathlib import Path
from bodhilib import (
    get_data_loader,
    get_splitter,
    get_embedder,
    get_vector_db,
    get_llm,
    Distance,
)

data_loader = get_data_loader("file")
splitter = get_splitter("text_splitter", max_len=300, overlap=30)
embedder = get_embedder("sentence_transformers")
vector_db = get_vector_db("qdrant", location=":memory:")
llm = get_llm('openai_chat', model='gpt-3.5-turbo')

# Get data directory path and add it to data_loader
current_dir = current_working_directory = Path(os.getcwd())
data_dir = current_dir / ".." / "data" / "data-loader"
data_loader.add_resource(dir=str(data_dir))
docs = data_loader.load()

# 
nodes = splitter.split(docs)

# 
_ = embedder.embed(nodes)

# 
collection_name = "test_collection"
if "test_collection" in vector_db.get_collections():
    vector_db.delete_collection("test_collection")
vector_db.create_collection(
    collection_name=collection_name,
    dimension=embedder.dimension,
    distance=Distance.COSINE,
)
_ = vector_db.upsert(collection_name, nodes)

# 
input_query = "According to Paul Graham, how to tackle when you are in doubt?"
embedding = embedder.embed(input_query)
result = vector_db.query(collection_name, embedding[0].embedding, limit=5)
```
:::

::: {.cell .code execution_count="3"}
``` python
# Create the prompt template for extracting answer from given text chunks
from bodhilib import PromptTemplate

template = """Below are the text chunks from a blog/article. 
1. Read and understand the text chunks
2. After the text chunks, there are list of questions starting with `Question:`
3. Answer the questions from the information given in the text chunks
4. If you don't find the answer in the provided text chunks, say 'I couldn't find the answer to this question in the given text'

{% for text in texts %}
### START
{{ text }}
### END
{% endfor %}

Question: {{ query }}
Answer: 
"""
prompt_template = PromptTemplate(template=template, format='jinja2')
```
:::

::: {.cell .code execution_count="4"}
``` python
texts = [r.text for r in result]
prompt = prompt_template.to_prompts(texts=texts, query=input_query)
```
:::

::: {.cell .code execution_count="6"}
``` python
# OpenAI API setup
import os
from getpass import getpass
from dotenv import load_dotenv

load_dotenv()
if "OPENAI_API_KEY" not in os.environ:
    os.environ["OPENAI_API_KEY"] = getpass("Enter your OpenAI API key: ")
```
:::

::: {.cell .code execution_count="7"}
``` python
# get the OpenAI LLM service instance
from bodhilib import get_llm

llm = get_llm('openai_chat', model='gpt-3.5-turbo')
```
:::

::: {.cell .code execution_count="8"}
``` python
response = llm.generate(prompt)
```
:::

::: {.cell .code execution_count="9"}
``` python
import textwrap

print(textwrap.fill(response.text, 100))
```

::: {.output .stream .stdout}
    According to Paul Graham, when you are in doubt, you should optimize for interestingness and give
    different types of work a chance to show you what they're like.
:::
:::

::: {.cell .markdown}

------------------------------------------------------------------------

ðŸŽ‰ We just created a flow for Extractive QnA using different bodhilib
components.

The Extractive QnA flow is so frequently used, that bodhiext provides an
implementation for this flow in form of BodhiEngine. Let\'s check out
[BodhiEngine](BodhiEngine.ipynb) next.
:::