{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG)\n",
    "\n",
    "## What is bodhilib?\n",
    "**bodhilib** is an Open Source (MIT License), Plugin Architecture based, Pythonic and Composable LLM Library.\n",
    "\n",
    "**Bodhi** is a Sanskrit term for deep insight into reality. With bodhilib, we aspire to provide tools for a deeper understanding of the data-rich world around us.\n",
    "\n",
    "### Plugin Architecture?\n",
    "**bodhilib** in itself, only defines the models and interfaces. All the implementations are provided by plugins installed separately.\n",
    "\n",
    "### Pythonic?\n",
    "**bodhilib** prefers Pythonic (over Java-like) syntax, uses the Python's dynamic language power, follows conservative in what you send, liberal in what you accept (Postel's Law).\n",
    "\n",
    "### Composable?\n",
    "The interfaces take inspiration from functional languages, to create composability.\n",
    "\n",
    "### LLM Library?\n",
    "**bodhilib** aspires to the LLM library of choice, developer-friendly, and feature rich through plugin extensions.\n",
    "\n",
    "---\n",
    "\n",
    "## What is Retrieval Augmented Generation (RAG)?\n",
    "Uses LLMs to retrieve relevant information from large corpus of data.\n",
    "\n",
    "## What are the components of RAG?\n",
    "1. **DataLoader** - loads the data from variety of sources\n",
    "1. **Splitter** - splits the **Documents** into processible entities **Node**\n",
    "1. **Embedder** - embeds the **Nodes** into a vector representation or **Embedding**\n",
    "1. **VectorDB** - stores the **Embedding**, along with metadata (*insert*), also retrieves based on similarity (*query*)\n",
    "1. **LLM** - a Large Language Model, to generate a response given an input or **Prompt**\n",
    "\n",
    "## How does ingestion work in RAG?\n",
    "1. **Data** is converted to **Documents** using **DataLoader**\n",
    "1. **Documents** are converted to **Nodes** using **Splitter**\n",
    "1. **Nodes** are enriched with **Embedding** using **Embedder**\n",
    "1. **Nodes** and **Embeddings** are inserted as **Records** using **VectorDB** *insert*\n",
    "\n",
    "![RAG Ingestion Pipeline](../images/rag-ingestion.png)\n",
    "\n",
    "## How does query work in RAG?\n",
    "1. **User** provides **Input Query**\n",
    "1. **Input Query** is converted into **Embedding** using **Embedder**\n",
    "1. **Embedding** is used to fetch similar **Records** using **VectorDB** *query*\n",
    "1. **Input Query** and **Records** are used to create **Prompt** using **PromptTemplate**\n",
    "1. **Prompt** is used to generate **Response** using **LLM**\n",
    "\n",
    "![RAG Query Pipeline](../images/rag-query.png)\n",
    "\n",
    "\n",
    "# RAG using bodhilib\n",
    "## 1. Installation\n",
    "Install the required libraries:\n",
    "\n",
    "1. `bodhilib` - the core library that defines the models and interfaces\n",
    "1. `bodhiext.openai` - the plugin implementing **LLM** interface for **OpenAI**\n",
    "1. `bodhiext.qdrant` - the plugin implementing **VectorDB** interface for **Qdrant**\n",
    "1. `bodhiext.sentence_transformers` - the plugin implementing **Embedder** interface to use **Sentence Transformers**\n",
    "1. `bodhiext.file` - the plugin implementing **DataLoader** interface to load local files (packaged with bodhilib)\n",
    "1. `bodhiext.text_splitter` - the plugin implementing **Splitter** interface to split based on sentences (packaged with bodhilib)\n",
    "1. `python-dotenv` - utility library to load environment variables from local `.env` file\n",
    "1. `fn.py` - Python library providing functional programming constructs, optional, used for Composability demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q bodhilib bodhiext.openai bodhiext.qdrant bodhiext.sentence_transformers python-dotenv fn.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility method\n",
    "import textwrap\n",
    "from reprlib import repr\n",
    "\n",
    "def wrap_text(text, width=100):\n",
    "    wrapped_lines = []\n",
    "    for line in text.splitlines():\n",
    "        wrapped_lines.extend(textwrap.fill(line, width=width).splitlines())\n",
    "    return '\\n'.join(wrapped_lines)\n",
    "\n",
    "def trim_text(text):\n",
    "    return repr(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize the Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bodhilib import (\n",
    "    Distance,\n",
    "    get_data_loader,\n",
    "    get_embedder,\n",
    "    get_llm,\n",
    "    get_splitter,\n",
    "    get_vector_db,\n",
    ")\n",
    "\n",
    "data_loader = get_data_loader(\"file\")\n",
    "splitter = get_splitter(\"text_splitter\", max_len=300, overlap=30)\n",
    "embedder = get_embedder(\"sentence_transformers\")\n",
    "vector_db = get_vector_db(\"qdrant\", location=\":memory:\")\n",
    "llm = get_llm('openai_chat', model='gpt-3.5-turbo')\n",
    "\n",
    "# recreate vectordb database\n",
    "collection_name = \"test_collection\"\n",
    "\n",
    "if \"test_collection\" in vector_db.get_collections():\n",
    "    vector_db.delete_collection(\"test_collection\")\n",
    "vector_db.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    dimension=embedder.dimension,\n",
    "    distance=Distance.COSINE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RAG Ingestion\n",
    "\n",
    "### 4.1 Load the data as Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "current_dir = Path(os.getcwd())\n",
    "data_dir = current_dir / \"..\" / \"data\" / \"data-loader\"\n",
    "data_loader.add_resource(dir=str(data_dir))\n",
    "docs = data_loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Split the Document into Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes = splitter.split(docs)\n",
    "len(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Enrich the Nodes with Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.046271130442619324, -0.0969996377825737, 0.09419207274913788, 0.014190234243869781, 0.02280914969742298, -0.01539283711463213, ...]\n"
     ]
    }
   ],
   "source": [
    "_ = embedder.embed(nodes)\n",
    "\n",
    "print(trim_text(nodes[0].embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Insert the Nodes into VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n"
     ]
    }
   ],
   "source": [
    "_ = vector_db.upsert(collection_name, nodes)\n",
    "\n",
    "print(vector_db.client.get_collection(collection_name).vectors_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RAG Query\n",
    "\n",
    "### 5.1 Input Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_query = \"According to Paul Graham, how to tackle when you are in doubt?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Embed the Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.043427370488643646, 0.03586525842547417, 0.00045173554099164903, -0.00947035662829876, -0.021341439336538315, 0.02608679234981537, ...]\n"
     ]
    }
   ],
   "source": [
    "query_embedding = embedder.embed(input_query)\n",
    "\n",
    "print(trim_text(query_embedding[0].embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Get Similar Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "who sits back and offers sophisticated-sounding criticisms of them. \"It's easy to criticize\" is true\n",
      "in the most literal sense, and the route to great work is never easy.\n",
      "There may be some jobs where it's an advantage to be cynical and pessimistic, but if you want to do\n",
      "great work it's an advantage to be optimistic, even though that means you'll risk looking like a\n",
      "fool sometimes. There's an old tradition of doing the opposite. The Old Testament says it's better\n",
      "to keep quiet lest you look like a fool. But that's advice for seeming smart. If you actually want\n",
      "to discover new things, it's better to take the risk of telling people your ideas.\n",
      "Some people are naturally earnest, and with others it takes a conscious effort. Either kind of\n",
      "earnestness will suffice. But I doubt it would be possible to do great work without being earnest.\n",
      "It's so hard to do even if you are. You don't have enough margin for error to accommodate the\n",
      "distortions introduced by being affected, intellectually dishonest, orthodox, fashionable, or cool.\n",
      "[14]\n",
      "Great work is consistent not only with who did it, but with itself. It's usually all of a piece. So\n",
      "if you face a decision in the middle of working on something, ask which choice is more consistent.\n",
      "You may have to throw things away and redo them. You won't necessarily have to, but you have to be\n",
      "willing to. And that can take some effort; when there's something you need to redo, status quo bias\n",
      "and laziness will combine to keep you in denial about it. To beat this ask: If I'd already made the\n",
      "change, would I want to revert to what I have now?\n",
      "Have the confidence to cut.\n"
     ]
    }
   ],
   "source": [
    "records = vector_db.query(collection_name, query_embedding[0].embedding, limit=5)\n",
    "\n",
    "print(wrap_text(records[0].text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Compose the Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare prompt template\n",
    "from bodhilib import PromptTemplate\n",
    "\n",
    "template = \"\"\"Below are the text chunks from a blog/article. \n",
    "1. Read and understand the text chunks\n",
    "2. After the text chunks, there are list of questions starting with `Question:`\n",
    "3. Answer the questions from the information given in the text chunks\n",
    "4. If you don't find the answer in the provided text chunks, say 'I couldn't find the answer to this question in the given text'\n",
    "\n",
    "\n",
    "{% for text in texts %}\n",
    "### START\n",
    "{{ text }}\n",
    "### END\n",
    "{% endfor %}\n",
    "\n",
    "Question: {{ query }}\n",
    "Answer: \n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(template=template, format='jinja2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below are the text chunks from a blog/article.\n",
      "1. Read and understand the text chunks\n",
      "2. After the text chunks, there are list of questions starting with `Question:`\n",
      "3. Answer the questions from the information given in the text chunks\n",
      "4. If you don't find the answer in the provided text chunks, say 'I couldn't find the answer to this\n",
      "question in the given text'\n",
      "### START\n",
      "who sits back and offers sophisticated-sounding criticisms of them. \"It's easy to criticize\" is true\n",
      "in the most literal sense, and the route to great work is never easy.\n",
      "There may be some jobs where it's an advantage to be cynical and pessimistic, but if you want to do\n",
      "great work it's an advantage to be optimistic, even though that means you'll risk looking like a\n",
      "fool sometimes. There's an old tradition of doing the opposite. The Old Testament says it's better\n",
      "to keep quiet lest you look like a fool. But that's advice for seeming smart. If you actually want\n",
      "to discover new things, it's better to take the risk of telling people your ideas.\n",
      "Some people are naturally earnest, and with others it takes a conscious effort. Either kind of\n",
      "earnestness will suffice. But I doubt it would be possible to do great work without being earnest.\n",
      "It's so hard to do even if you are. You don't have enough margin for error to accommodate the\n",
      "distortions introduced by being affected, intellectually dishonest, orthodox, fashionable, or cool.\n",
      "[14]\n",
      "Great work is consistent not only with who did it, but with itself. It's usually all of a piece. So\n",
      "if you face a decision in the middle of working on something, ask which choice is more consistent.\n",
      "You may have to throw things away and redo them. You won't necessarily have to, but you have to be\n",
      "willing to. And that can take some effort; when there's something you need to redo, status quo bias\n",
      "and laziness will combine to keep you in denial about it. To beat this ask: If I'd already made the\n",
      "change, would I want to revert to what I have now?\n",
      "Have the confidence to cut.\n",
      "### END\n",
      "### START\n",
      "gaps.\n",
      "The next step is to notice them. This takes some skill, because your brain wants to ignore such gaps\n",
      "in order to make a simpler model of the world. Many discoveries have come from asking questions\n",
      "about things that everyone else took for granted. [2]\n",
      "If the answers seem strange, so much the better. Great work often has a tincture of strangeness. You\n",
      "see this from painting to math. It would be affected to try to manufacture it, but if it appears,\n",
      "embrace it.\n",
      "Boldly chase outlier ideas, even if other people aren't interested in them — in fact, especially if\n",
      "they aren't. If you're excited about some possibility that everyone else ignores, and you have\n",
      "enough expertise to say precisely what they're all overlooking, that's as good a bet as you'll find.\n",
      "[3]\n",
      "Four steps: choose a field, learn enough to get to the frontier, notice gaps, explore promising\n",
      "ones. This is how practically everyone who's done great work has done it, from painters to\n",
      "physicists.\n",
      "Steps two and four will require hard work. It may not be possible to prove that you have to work\n",
      "hard to do great things, but the empirical evidence is on the scale of the evidence for mortality.\n",
      "That's why it's essential to work on something you're deeply interested in. Interest will drive you\n",
      "to work harder than mere diligence ever could.\n",
      "The three most powerful motives are curiosity, delight, and the desire to do something impressive.\n",
      "Sometimes they converge, and that combination is the most powerful of all.\n",
      "The big prize is to discover a new fractal bud. You notice a crack in the surface of knowledge, pry\n",
      "it open, and there's a whole world inside.\n",
      "### END\n",
      "### START\n",
      "around with you. But the more you're carrying, the greater the chance of noticing a solution — or\n",
      "perhaps even more excitingly, noticing that two unanswered questions are the same.\n",
      "Sometimes you carry a question for a long time. Great work often comes from returning to a question\n",
      "you first noticed years before — in your childhood, even — and couldn't stop thinking about. People\n",
      "talk a lot about the importance of keeping your youthful dreams alive, but it's just as important to\n",
      "keep your youthful questions alive. [19]\n",
      "This is one of the places where actual expertise differs most from the popular picture of it. In the\n",
      "popular picture, experts are certain. But actually the more puzzled you are, the better, so long as\n",
      "(a) the things you're puzzled about matter, and (b) no one else understands them either.\n",
      "Think about what's happening at the moment just before a new idea is discovered. Often someone with\n",
      "sufficient expertise is puzzled about something. Which means that originality consists partly of\n",
      "puzzlement — of confusion! You have to be comfortable enough with the world being full of puzzles\n",
      "that you're willing to see them, but not so comfortable that you don't want to solve them. [20]\n",
      "It's a great thing to be rich in unanswered questions. And this is one of those situations where the\n",
      "rich get richer, because the best way to acquire new questions is to try answering existing ones.\n",
      "Questions don't just lead to answers, but also to more questions.\n",
      "The best questions grow in the answering. You notice a thread protruding from the current paradigm\n",
      "and try pulling on it, and it just gets longer and longer. So don't require a question to be\n",
      "obviously big before you try answering it. You can rarely predict that.\n",
      "### END\n",
      "### START\n",
      "assumption that you'll somehow magically guess as a teenager. They don't tell you, but I will: when\n",
      "it comes to figuring out what to work on, you're on your own. Some people get lucky and do guess\n",
      "correctly, but the rest will find themselves scrambling diagonally across tracks laid down on the\n",
      "assumption that everyone does.\n",
      "What should you do if you're young and ambitious but don't know what to work on? What you should not\n",
      "do is drift along passively, assuming the problem will solve itself. You need to take action. But\n",
      "there is no systematic procedure you can follow. When you read biographies of people who've done\n",
      "great work, it's remarkable how much luck is involved. They discover what to work on as a result of\n",
      "a chance meeting, or by reading a book they happen to pick up. So you need to make yourself a big\n",
      "target for luck, and the way to do that is to be curious. Try lots of things, meet lots of people,\n",
      "read lots of books, ask lots of questions. [5]\n",
      "When in doubt, optimize for interestingness. Fields change as you learn more about them. What\n",
      "mathematicians do, for example, is very different from what you do in high school math classes. So\n",
      "you need to give different types of work a chance to show you what they're like. But a field should\n",
      "become increasingly interesting as you learn more about it. If it doesn't, it's probably not for\n",
      "you.\n",
      "Don't worry if you find you're interested in different things than other people. The stranger your\n",
      "tastes in interestingness, the better. Strange tastes are often strong ones, and a strong taste for\n",
      "work means you'll be productive.\n",
      "### END\n",
      "### START\n",
      "about, so we can ignore that. And we can assume effort, if you do in fact want to do great work. So\n",
      "the problem boils down to ability and interest. Can you find a kind of work where your ability and\n",
      "interest will combine to yield an explosion of new ideas?\n",
      "Here there are grounds for optimism. There are so many different ways to do great work, and even\n",
      "more that are still undiscovered. Out of all those different types of work, the one you're most\n",
      "suited for is probably a pretty close match. Probably a comically close match. It's just a question\n",
      "of finding it, and how far into it your ability and interest can take you. And you can only answer\n",
      "that by trying.\n",
      "Many more people could try to do great work than do. What holds them back is a combination of\n",
      "modesty and fear. It seems presumptuous to try to be Newton or Shakespeare. It also seems hard;\n",
      "surely if you tried something like that, you'd fail. Presumably the calculation is rarely explicit.\n",
      "Few people consciously decide not to try to do great work. But that's what's going on\n",
      "subconsciously; they shy away from the question.\n",
      "So I'm going to pull a sneaky trick on you. Do you want to do great work, or not? Now you have to\n",
      "decide consciously. Sorry about that. I wouldn't have done it to a general audience. But we already\n",
      "know you're interested.\n",
      "Don't worry about being presumptuous. You don't have to tell anyone. And if it's too hard and you\n",
      "fail, so what? Lots of people have worse problems than that. In fact you'll be lucky if it's the\n",
      "worst problem you have.\n",
      "Yes, you'll have to work hard. But again, lots of people have to work hard.\n",
      "### END\n",
      "Question: According to Paul Graham, how to tackle when you are in doubt?\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "# compose the prompt\n",
    "texts = [r.text for r in records]\n",
    "prompt = prompt_template.to_prompts(texts=texts, query=input_query)\n",
    "\n",
    "print(wrap_text(prompt[0].text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Generate Response from LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the text, when in doubt, it is recommended to optimize for interestingness.\n"
     ]
    }
   ],
   "source": [
    "response = llm.generate(prompt)\n",
    "\n",
    "print(wrap_text(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Benefits of Bodhilib\n",
    "\n",
    "## 1. Benefits of Plugin Architecture\n",
    "\n",
    "- Modular and clean core\n",
    "- Easy to understand, decluttered core\n",
    "- Easy to extend for specific use-case using custom plugins\n",
    "- Interchangeable components, uniform interface\n",
    "- Selective integration, install only what you need\n",
    "- Democratic and Distributed development, no single-repo for all implementation\n",
    "- No PR queue on single repo, open-issues, few core-committers\n",
    "- Stable core library, scheduled releases\n",
    "- Independent plugin library fixes and releases\n",
    "- No preferred partner integration, common interface for 3rd party to implement\n",
    "- No pay-wall, plus-offering, walled-garden approach\n",
    "- No re-inventing the wheel, cornered custom eco-system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Composable Functional Interface\n",
    "\n",
    "See [Composability](../deep-dive/Composability.ipynb) notebook for the full example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to Paul Graham, when in doubt, one should optimize for interestingness and give different\n",
      "types of work a chance to show what they're like.\n"
     ]
    }
   ],
   "source": [
    "from fn import F # fn.py\n",
    "\n",
    "if \"test_collection\" in vector_db.get_collections():\n",
    "    vector_db.delete_collection(\"test_collection\")\n",
    "vector_db.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    dimension=embedder.dimension,\n",
    "    distance=Distance.COSINE,\n",
    ")\n",
    "\n",
    "# RAG Ingestion Pipeline\n",
    "f = (\n",
    "    F(data_loader.load)\n",
    "    >> F(splitter.split)\n",
    "    >> F(embedder.embed)\n",
    "    >> F(lambda nodes: vector_db.upsert(collection_name=collection_name, nodes=nodes))\n",
    ")()\n",
    "\n",
    "# RAG Query Pipeline\n",
    "response = (\n",
    "    F(embedder.embed)\n",
    "    >> F(\n",
    "        lambda e: vector_db.query(\n",
    "            collection_name=collection_name, embedding=e[0].embedding, limit=5\n",
    "        )\n",
    "    )\n",
    "    >> F(lambda nodes: prompt_template.to_prompts(query=input_query, texts = [node.text for node in nodes]))\n",
    "    >> F(llm.generate)\n",
    ")(input_query)\n",
    "\n",
    "print(wrap_text(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Roadmap and Contributing\n",
    "\n",
    "Refer to [Roadmap](../contributing/roadmap.md) page for draft roadmap and contributing to Bodhilib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contact\n",
    "\n",
    "Github: [https://github.com/BodhiSearch/bodhilib](https://github.com/BodhiSearch/bodhilib)\n",
    "\n",
    "\n",
    "Guide: [https://github.com/BodhiSearch/bodhilib-guide](https://github.com/BodhiSearch/bodhilib-guide)\n",
    "\n",
    "\n",
    "Follow on Twitter [@BodhilibAI](https://twitter.com/BodhilibAI)\n",
    "\n",
    "\n",
    "Join us on Bodhilib WhatsApp Community - [https://bit.ly/bodhilib-wa](https://bit.ly/bodhilib-wa)\n",
    "\n",
    "\n",
    "![Bodhilib WhatsApp Community](../images/bodhilib-wa.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "🙏🏽 Thanks 🙏🏽"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bodhi-guide",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
