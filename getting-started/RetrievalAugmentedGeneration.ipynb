{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG)\n",
    "\n",
    "## What is bodhilib?\n",
    "**bodhilib** is an **Open Source** (MIT License), **Plugin Architecture** based, **Pythonic** and **Composable LLM Library**.\n",
    "\n",
    "**Bodhi** is a Sanskrit term for deep insight into reality. With bodhilib, we aspire to provide tools for a deeper understanding of the data-rich world around us.\n",
    "\n",
    "### Plugin Architecture?\n",
    "**bodhilib** in itself, only defines the models and interfaces. All the implementations are provided by plugins installed separately.\n",
    "\n",
    "### Pythonic?\n",
    "**bodhilib** prefers Pythonic (over Java-like) syntax, uses the Python's dynamic language power.\n",
    "\n",
    "### Composable?\n",
    "The interfaces take inspiration from functional languages (specially Haskell), to create composability among components. \n",
    "Follows Postel's Law - conservative in what you send, liberal in what you accept.\n",
    "\n",
    "### LLM Library?\n",
    "**bodhilib** aspires to the LLM library of choice, developer-friendly, and feature rich, through distributed and democratic development using plugin architecture.\n",
    "\n",
    "---\n",
    "\n",
    "## What is Retrieval Augmented Generation (RAG)?\n",
    "Uses LLMs to retrieve relevant information from large corpus of data.\n",
    "\n",
    "## What are the components of RAG?\n",
    "1. **DataLoader** - loads the data from sources as **Documents**\n",
    "1. **Splitter** - splits the **Documents** into processible entities **Node**\n",
    "1. **Embedder** - embeds the **Nodes** into a vector representation or **Embedding**\n",
    "1. **VectorDB** - stores the **Embedding**, along with metadata (*insert*), also retrieves based on similarity (*query*)\n",
    "1. **LLM** - a Large Language Model, takes a user input or **Prompt** to generate a response\n",
    "\n",
    "## How does ingestion work in RAG?\n",
    "1. **Data** is converted to **Documents** using **DataLoader**\n",
    "1. **Documents** are converted to **Nodes** using **Splitter**\n",
    "1. **Nodes** are enriched with **Embedding** using **Embedder**\n",
    "1. **Nodes** and **Embeddings** are inserted as **Records** using **VectorDB** *insert*\n",
    "\n",
    "![RAG Ingestion Pipeline](../images/rag-ingestion.png)\n",
    "\n",
    "## How does query work in RAG?\n",
    "1. **User** provides **Input Query**\n",
    "1. **Input Query** is converted into **Embedding** using **Embedder**\n",
    "1. **Embedding** is used to fetch similar **Records** using **VectorDB** *query*\n",
    "1. **Input Query** and **Records** are used to create **Prompt** using **PromptTemplate**\n",
    "1. **Prompt** is used to generate **Response** using **LLM**\n",
    "\n",
    "![RAG Query Pipeline](../images/rag-query.png)\n",
    "\n",
    "\n",
    "# RAG using bodhilib\n",
    "## 1. Installation\n",
    "Install the required libraries:\n",
    "\n",
    "1. `bodhilib` - the core library that defines the models and interfaces\n",
    "1. `bodhiext.openai` - the plugin implementing **LLM** interface for **OpenAI**\n",
    "1. `bodhiext.qdrant` - the plugin implementing **VectorDB** interface for **Qdrant**\n",
    "1. `bodhiext.sentence_transformers` - the plugin implementing **Embedder** interface to use **Sentence Transformers**\n",
    "1. `bodhiext.file` - the plugin implementing **DataLoader** interface to load local files (packaged with bodhilib)\n",
    "1. `bodhiext.text_splitter` - the plugin implementing **Splitter** interface to split based on sentences (packaged with bodhilib)\n",
    "1. `python-dotenv` - utility library to load environment variables from local `.env` file\n",
    "1. `fn.py` - Python library providing functional programming constructs, optional, used for Composability demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q bodhilib bodhiext.openai bodhiext.qdrant bodhiext.sentence_transformers python-dotenv fn.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility method\n",
    "import textwrap\n",
    "from reprlib import repr\n",
    "\n",
    "def wrap_text(text, width=100):\n",
    "    wrapped_lines = []\n",
    "    for line in text.splitlines():\n",
    "        wrapped_lines.extend(textwrap.fill(line, width=width).splitlines())\n",
    "    return '\\n'.join(wrapped_lines)\n",
    "\n",
    "def trim_text(text):\n",
    "    return repr(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize the Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bodhilib import (\n",
    "    Distance,\n",
    "    get_data_loader,\n",
    "    get_embedder,\n",
    "    get_llm,\n",
    "    get_splitter,\n",
    "    get_vector_db,\n",
    ")\n",
    "\n",
    "data_loader = get_data_loader(\"file\")\n",
    "splitter = get_splitter(\"text_splitter\", max_len=300, overlap=30)\n",
    "embedder = get_embedder(\"sentence_transformers\")\n",
    "vector_db = get_vector_db(\"qdrant\", location=\":memory:\")\n",
    "llm = get_llm(\"openai_chat\", model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# recreate vectordb database\n",
    "collection_name = \"test_collection\"\n",
    "\n",
    "if \"test_collection\" in vector_db.get_collections():\n",
    "    vector_db.delete_collection(\"test_collection\")\n",
    "vector_db.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    dimension=embedder.dimension,\n",
    "    distance=Distance.COSINE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RAG Ingestion\n",
    "\n",
    "### 4.1 Load the data as Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "current_dir = Path(os.getcwd())\n",
    "data_dir = current_dir / \"..\" / \"data\" / \"data-loader\"\n",
    "data_loader.add_resource(dir=str(data_dir))\n",
    "docs = data_loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Split the Document into Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = splitter.split(docs)\n",
    "len(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Enrich the Nodes with Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = embedder.embed(nodes)\n",
    "\n",
    "print(trim_text(nodes[0].embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Insert the Nodes into VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = vector_db.upsert(collection_name, nodes)\n",
    "\n",
    "print(vector_db.client.get_collection(collection_name).vectors_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RAG Query\n",
    "\n",
    "### 5.1 Input Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_query = \"According to Paul Graham, how to tackle when you are in doubt?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Embed the Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embedding = embedder.embed(input_query)\n",
    "\n",
    "print(trim_text(query_embedding[0].embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Get Similar Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = vector_db.query(collection_name, query_embedding[0].embedding, limit=5)\n",
    "\n",
    "print(wrap_text(records[0].text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Compose the Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare prompt template\n",
    "from bodhilib import PromptTemplate\n",
    "\n",
    "template = \"\"\"Below are the text chunks from a blog/article. \n",
    "1. Read and understand the text chunks\n",
    "2. After the text chunks, there are list of questions starting with `Question:`\n",
    "3. Answer the questions from the information given in the text chunks\n",
    "4. If you don't find the answer in the provided text chunks, say 'I couldn't find the answer to this question in the given text'\n",
    "\n",
    "\n",
    "{% for text in texts %}\n",
    "### START\n",
    "{{ text }}\n",
    "### END\n",
    "{% endfor %}\n",
    "\n",
    "Question: {{ query }}\n",
    "Answer: \n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(template=template, format='jinja2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compose the prompt\n",
    "texts = [r.text for r in records]\n",
    "prompt = prompt_template.to_prompts(texts=texts, query=input_query)\n",
    "\n",
    "print(wrap_text(prompt[0].text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Generate Response from LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.generate(prompt)\n",
    "\n",
    "print(wrap_text(\"Question: \" + input_query))\n",
    "print(wrap_text(\"Answer: \" + response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Benefits of Bodhilib\n",
    "\n",
    "## 1. Plugin Architecture\n",
    "\n",
    "- Clean and concise core\n",
    "- Easy to understand\n",
    "- Extensible using plugins\n",
    "- Interchangeable components, uniform interface\n",
    "- Selective integration, install only what you need\n",
    "- Democratic and Distributed development, no single-repo for all implementation\n",
    "- No PR queue on single repo, open-issues, few core-committers\n",
    "- Stable core library, scheduled releases\n",
    "- Independent plugin library fixes and releases\n",
    "- Batteries in the box with `bodhiext.*` packages, implementing interface for popular components\n",
    "- No preferred partner integration, common interface for 3rd party to implement\n",
    "- No pay-wall, plus-offering, walled-garden approach\n",
    "- No re-inventing the wheel, cornered custom eco-system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Composable Functional Interface\n",
    "\n",
    "See [Composability](../deep-dive/Composability.ipynb) notebook for the full example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fn import F # fn.py\n",
    "\n",
    "if \"test_collection\" in vector_db.get_collections():\n",
    "    vector_db.delete_collection(\"test_collection\")\n",
    "vector_db.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    dimension=embedder.dimension,\n",
    "    distance=Distance.COSINE,\n",
    ")\n",
    "\n",
    "# RAG Ingestion Pipeline\n",
    "f = (\n",
    "    F(data_loader.load)\n",
    "    >> F(splitter.split)\n",
    "    >> F(embedder.embed)\n",
    "    >> F(lambda nodes: vector_db.upsert(collection_name=collection_name, nodes=nodes))\n",
    ")()\n",
    "\n",
    "# RAG Query Pipeline\n",
    "response = (\n",
    "    F(embedder.embed)\n",
    "    >> F(\n",
    "        lambda e: vector_db.query(\n",
    "            collection_name=collection_name, embedding=e[0].embedding, limit=5\n",
    "        )\n",
    "    )\n",
    "    >> F(lambda nodes: prompt_template.to_prompts(query=input_query, texts = [node.text for node in nodes]))\n",
    "    >> F(llm.generate)\n",
    ")(input_query)\n",
    "\n",
    "print(wrap_text(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Roadmap and Contributing\n",
    "\n",
    "Refer to [Roadmap](../contributing/roadmap.md) page for draft roadmap and contributing to Bodhilib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contact\n",
    "\n",
    "Github: [https://github.com/BodhiSearch/bodhilib](https://github.com/BodhiSearch/bodhilib)\n",
    "\n",
    "\n",
    "Guide: [https://github.com/BodhiSearch/bodhilib-guide](https://github.com/BodhiSearch/bodhilib-guide)\n",
    "\n",
    "\n",
    "Follow on Twitter [@BodhilibAI](https://twitter.com/BodhilibAI)\n",
    "\n",
    "\n",
    "Join us on Bodhilib WhatsApp Community - [https://bit.ly/bodhilib-wa](https://bit.ly/bodhilib-wa)\n",
    "\n",
    "\n",
    "![Bodhilib WhatsApp Community](../images/bodhilib-wa.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "üôèüèΩ Thanks üôèüèΩ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bodhi-guide",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
