{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractive QnA\n",
    "\n",
    "So far, we have seen how to load our documents, split them into Nodes, create Embeddings for the Nodes, insert them into Vector database, and then finally query them for a given input query.\n",
    "\n",
    "In this Getting Started guide, we are going to see how to use LLM to do an extractive QnA on the returned nodes, and display result directly. We are going to use service and components that we have already covered in the guide to build this workflow.\n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the node embeddings for the Paul Graham's essay:\n",
    "# 1. Load the Paul Graham essays from data/data-loader directory using `file` DataLoader\n",
    "# 2. Convert it into Nodes using sentence_splitter\n",
    "# 3. Enrich node embeddings using the sentence_transformers\n",
    "import os\n",
    "from pathlib import Path\n",
    "from bodhilib import (\n",
    "    get_data_loader,\n",
    "    get_splitter,\n",
    "    get_embedder,\n",
    "    get_vector_db,\n",
    "    Distance,\n",
    ")\n",
    "\n",
    "# Get data directory path and add it to data_loader\n",
    "current_dir = current_working_directory = Path(os.getcwd())\n",
    "data_dir = current_dir / \"..\" / \"data\" / \"data-loader\"\n",
    "data_loader = get_data_loader(\"file\")\n",
    "data_loader.add_resource(dir=str(data_dir))\n",
    "docs = data_loader.load()\n",
    "splitter = get_splitter(\"text_splitter\", max_len=300, overlap=30)\n",
    "nodes = splitter.split(docs)\n",
    "embedder = get_embedder(\"sentence_transformers\")\n",
    "_ = embedder.embed(nodes)\n",
    "collection_name = \"test_collection\"\n",
    "vector_db = get_vector_db(\"qdrant\", location=\":memory:\")\n",
    "if \"test_collection\" in vector_db.get_collections():\n",
    "    vector_db.delete_collection(\"test_collection\")\n",
    "vector_db.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    dimension=embedder.dimension,\n",
    "    distance=Distance.COSINE,\n",
    ")\n",
    "_ = vector_db.upsert(collection_name, nodes)\n",
    "input_query = \"According to Paul Graham, how to tackle when you are in doubt?\"\n",
    "embedding = embedder.embed(input_query)\n",
    "result = vector_db.query(collection_name, embedding[0].embedding, limit=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the prompt template for extracting answer from given text chunks\n",
    "from bodhilib import PromptTemplate\n",
    "\n",
    "template = \"\"\"Below are the text chunks from a blog/article. \n",
    "1. Read and understand the text chunks\n",
    "2. After the text chunks, there are list of questions starting with `Question:`\n",
    "3. Answer the questions from the information given in the text chunks\n",
    "4. If you don't find the answer in the provided text chunks, say 'I couldn't find the answer to this question in the given text'\n",
    "\n",
    "{% for text in texts %}\n",
    "### START\n",
    "{{ text }}\n",
    "### END\n",
    "{% endfor %}\n",
    "\n",
    "Question: {{ query }}\n",
    "Answer: \n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(template=template, format='jinja2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [r.text for r in result]\n",
    "prompt = prompt_template.to_prompts(texts=texts, query=input_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API setup\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if 'OPENAI_API_KEY' not in os.environ:\n",
    "    os.environ['OPENAI_API_KEY'] = getpass('Enter your OpenAI API key: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the OpenAI LLM service instance\n",
    "from bodhilib import get_llm\n",
    "\n",
    "llm = get_llm('openai_chat', model='gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.generate(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the given text chunks, when you are in doubt, you should optimize for interestingness and give different types of work a chance to show you what they're like. You should also try lots of things, meet lots of people, read lots of books, and ask lots of questions.\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "ðŸŽ‰ We just created a flow for Extractive QnA using different bodhilib components.\n",
    "\n",
    "The Extractive QnA flow is so frequently used, that bodhiext provides an implementation for this flow in form of BodhiEngine. Let's check out [BodhiEngine](BodhiEngine) next."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bodhilib-guide",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
