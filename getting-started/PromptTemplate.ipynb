{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PromptTemplate\n",
    "\n",
    "PromptTemplate is used to generate dynamic prompts by injecting variables at runtime and creating parameterized prompts.\n",
    "\n",
    "Bodhilib supports 2 ways of injecting variables using PromptTemplate -\n",
    "\n",
    "1. fstring\n",
    "\n",
    "    [fstring](https://peps.python.org/pep-0498/) uses the python's native string interpolation features to inject named variables. \n",
    "    \n",
    "    In `fstring`, you define your named variable using single curly braces, `{` and `}`, and during variable injection, the value is replaced by the variable value passed at runtime.\n",
    "    \n",
    "    For e.g. for a fstring template - \"The {movie} is the best super-hero movie ever.\". At runtime, the `{movie}` is going to be replaced by the value of variable `movie` and produce a dynamic string to be used as prompt.\n",
    "\n",
    "2. jinja2\n",
    "\n",
    "    Though powerful, `fstring` is limited in its features. It cannot have `for-loops`, `if-else conditionals` etc. To overcome these limitations, bodhilib also supports [jinja2](https://palletsprojects.com/p/jinja/) as a templating engine.\n",
    "\n",
    "    In `jinja2` format, you define your prompt template using the [jinja2 format](https://palletsprojects.com/p/jinja/), and then these templates are safely injected with variables at runtime to produce the dynamic prompt.\n",
    "\n",
    "\n",
    "While creating the PromptTemplate, you specify which format you want to use using the `format` parameter. Then PromptTemplate uses that templating engine to inject variables and generate the Prompt.\n",
    "\n",
    "Let's try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PromptTemplate class\n",
    "\n",
    "from bodhilib import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the template using fstring format\n",
    "\n",
    "template = PromptTemplate(template=\"The {movie} is the best super-hero movie of all times. Do you agree?\", format=\"fstring\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = [\"Avengers: End Game\", \"Justice League\", \"The Dark Knight\"]\n",
    "\n",
    "# Use the PromptTemplate `to_prompts` method to generate the prompts\n",
    "for option in options:\n",
    "    print(\">\", template.to_prompts(movie=option))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pass these prompts to llm and see what it thinks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the environment variables\n",
    "# input your OpenAI API key when prompted\n",
    "\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if 'OPENAI_API_KEY' not in os.environ:\n",
    "    os.environ['OPENAI_API_KEY'] = getpass('Enter your API key: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get instance of OpenAI Chat LLM service\n",
    "from bodhilib import get_llm\n",
    "\n",
    "llm = get_llm(\"openai_chat\", model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "# Use the PromptTemplate `to_prompts` method to generate the prompts\n",
    "for option in options:\n",
    "    prompts = template.to_prompts(movie=option)\n",
    "    response = llm.generate(prompts)\n",
    "    print(f\"> {prompts[0].text}\\n\")\n",
    "    print(textwrap.fill(response.text, 120))\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "ðŸŽ‰ We just generated response from LLM using our dynamically generated prompts using PromptTemplate.\n",
    "\n",
    "Next, let's look at generating a more complex prompt that involves looping through options to generate a Prompt. We will use `jinja2` format to create this complex prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Which is the best super-hero movie of all times from among the options below? and why?\n",
    "\n",
    "{% for option in options -%}\n",
    "{{ loop.index }}. {{ option }}\n",
    "{% endfor -%}\n",
    "\"\"\"\n",
    "template = PromptTemplate(template=template, format=\"jinja2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = template.to_prompts(options=options)\n",
    "print(prompts[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what our LLM model thinks about the above. Let's pass the prompts to LLM service and get the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "response = llm.generate(prompts)\n",
    "print(textwrap.fill(response.text, 120))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "ðŸŽ‰ Yay! we have our our first response from LLM using a complex dynamic prompt generated using `jinja2` template.\n",
    "\n",
    "Next, we look at `PromptSource` and finding the right prompt for our use case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bodhi-guide",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
