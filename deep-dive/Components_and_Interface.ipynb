{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bodhilib - Models, Components and Interfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core models of bodhilib are -\n",
    "\n",
    "1. Prompt\n",
    "1. PromptStream\n",
    "1. PromptTemplate\n",
    "1. Document\n",
    "1. Node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core components of bodhilib are -\n",
    "1. DataLoader\n",
    "1. Splitter\n",
    "1. Embedder\n",
    "1. PromptSource\n",
    "1. VectorDB\n",
    "1. LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt and PromptStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Role(str, Enum):\n",
    "    SYSTEM = \"system\"\n",
    "    AI = \"ai\"\n",
    "    USER = \"user\"\n",
    "\n",
    "class Source(str, Enum):\n",
    "    INPUT = \"input\"\n",
    "    OUTPUT = \"output\"\n",
    "\n",
    "class SupportsText(Protocol):\n",
    "    @property\n",
    "    def text(self) -> str: ...\n",
    "\n",
    "class Prompt(BaseModel):\n",
    "    text: str\n",
    "    role: Role\n",
    "    source: Source\n",
    "    extras: Dict[str, Any]\n",
    "\n",
    "class PromptStream(Iterator[Prompt]):\n",
    "    def __iter__(self) -> Iterator[Prompt]: ...\n",
    "    def __next__(self) -> Prompt: ...\n",
    "    @property\n",
    "    def text(self) -> str: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Engine = Literal[\"default\", \"jinja2\"]\n",
    "\n",
    "class PromptTemplate:\n",
    "    def __init__(\n",
    "        self,\n",
    "        template: str,\n",
    "        engine: Optional[Engine] = \"default\",\n",
    "    ) -> None: ...\n",
    "\n",
    "    def to_prompt(self, **kwargs: Dict[str, Any]) -> [Prompt]: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PromptTemplate` allows you to generate prompt for your use-case injecting it with the right context. It re-uses the rich eco-system of python, and does not re-invent the wheel in the process. \n",
    "\n",
    "PromptTemplate supports 4 formats:\n",
    "1. `fstring`\n",
    "\n",
    "    For simple prompts involving variable injection, you can use the `fstring` format. It uses python's native f-string formatting and interpolation to inject your variables. You can then pass your variables to the `to_prompt` method to build your prompt.\n",
    "\n",
    "2. `jinja2`\n",
    "\n",
    "    For more complex prompts involving `loop`, `if-else` conditionals, you can uses `jinja2` templating library, and pass the template as a `jinja2` compatible template. You can then pass your variables to the `to_prompt` method build your prompt.\n",
    "\n",
    "3. `bodhilib-fstring`\n",
    "\n",
    "    `bodhilib-fstring` allows you to load simple prompts using `PromptSource` component. The prompts are serialized in bodhilib-prompt-template format, and uses `f-string` format for variable injections. Check out `PromptSource` component for details.\n",
    "\n",
    "4. `bodhilib-jinja2`\n",
    "\n",
    "    `bodhilib-jinja2` allows you to load complex prompts using `PromptSource` component. The prompts are serialized in bodhilib-prompt-template format, and uses `jinja2` templates for variable injections. Check out `PromptSource` component for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document and Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Embedding: TypeAlias = List[float]\n",
    "\n",
    "class Document(BaseModel):\n",
    "    text: str\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "class Node(BaseModel):\n",
    "    id: Optional[str]\n",
    "    text: str\n",
    "    parent: Optional[Document]\n",
    "    metadata: Dict[str, Any]\n",
    "    embedding: Optional[Embedding]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialized Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupportsText(Protocol):\n",
    "    @property\n",
    "    def text(self) -> str: ...\n",
    "\n",
    "SerializedInput: TypeAlias = str | List[str] | SupportsText | List[SupportsText] | Dict[str, Any] | List[Dict[str, Any]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SupportsText` protocol is a python protocol. So you don't need to explicitly implement it. If you have a property text in the object, then you automatically support this protocol. All the main models - `Prompt`, `PromptStream`, `Document` and `Node` supports this protocol by having a property `text` that contains the main content.\n",
    "\n",
    "This way, we have a very fluid composability. We can either pass in a `str`, or for processing a list of texts we can pass in a `List[str]`, or pass in `SupportsText` or a `List[SupportsText]` that can be any of `Prompt`, `PromptStream`, `Document` or `Node`, or a dict representation of object containing text property like `{\"text\": \"your content\"}` or a list of such serialized representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader\n",
    "\n",
    "A DataLoader is configured using the `add_resource` method. Once configured, it can be either iterated to fetch the resources as `Document` on-demand, or eager fetched using the `load` method to get it as a `List[Document]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(Iterable[Document], abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def add_resource(self, **kwargs: Dict[str, Any]) -> None: ...\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def __iter__(self) -> Iterator[Document]: ...\n",
    "\n",
    "    def load(self) -> List[Document]: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitter\n",
    "\n",
    "Splitter is used to split `Document` into right-sized processible chunks. For flexibility and composability, it takes in `SerializedInput`, and returns a list of `Node` with text corresponding to splits done by the implementation.\n",
    "\n",
    "Ideally, you pass in `Document` or a list of `Document` to get back a list of `Node` split into processible chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Splitter(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def split(self, inputs: SerializedInput) -> List[Node]: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedder\n",
    "\n",
    "Embedder takes in `SerializedInput`, and returns a list of `Node` enriched with `embedding`. If you pass in the `Node` or `List[Node]`, the passed argument itself is enriched with `embedding`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def embed(self, inputs: SerializedInput) -> List[Node]: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PromptSource\n",
    "\n",
    "`PromptSource` provides you an interface to browse and search through collection of most effective prompts. This way, you can test multiple prompt templates for your use-case and find the one that works for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptSource(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def find(self, keywords: str | List[str]) -> List[PromptTemplate]: ...\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def list_all(self) -> List[PromptTemplate]: ...    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorDB(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def upsert(self, collection_name: str, nodes: List[Node]) -> List[Node]: ...\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def query(\n",
    "        self, collection_name: str, embedding: Embedding, filter: Optional[Dict[str, Any]], **kwargs: Dict[str, Any]\n",
    "    ) -> List[Node]: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VectorDB has two main interface - `upsert` and `query`.\n",
    "\n",
    "`upsert` takes in a list of `Node`, and inserts or update the underlying VectorDB with the `text`, `metadata` and the `embedding` in  the `Node` object. These can later be used to query based on property or vector search.\n",
    "\n",
    "`query` method allows you to query the underlying vector database with the given embedding and property filters. The property filters uses the `MongoDB` query syntax, and not tied to specific vector database. These property filters are transformed by the `VectorDB` to the database specific filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLM(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def generate(\n",
    "        self,\n",
    "        prompt_input: SerializedInput,\n",
    "        *,\n",
    "        stream: Optional[bool] = None,\n",
    "        **kwargs: Dict[str, Any]) -> Union[Prompt, PromptStream]: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LLM has method generate that takes in a flexible `SerializedInput` to return either of `Prompt` or `PromptStream` depending if you are passing `stream` a False or True respectively.\n",
    "\n",
    "So, any of the following calls are valid and will generate you a response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.generate(\"tell me a joke\")\n",
    "llm.generate([\"tell me a joke\", \"joke should be related to architects\"])\n",
    "llm.generate(Prompt(\"tell me a joke\"))\n",
    "llm.generate([Prompt(\"you are a helpful AI assistant.\", role=\"system\"), Prompt(\"tell me a joke\")])\n",
    "llm.generate({\"text\": \"tell me a joke\", \"role\": \"user\"})\n",
    "llm.generate([{\"text\": \"you are a helpful AI assistant.\", \"role\": \"system\"}, {\"text\": \"tell me a joke\", \"role\": \"user\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Composability and RAG use-case\n",
    "\n",
    "`bodhilib` library is designed with composability in mind. It takes many ideas from strict functional languages like `Haskell` to design and implement its interface.\n",
    "\n",
    "Using the bodhilib library, you can simplify the ingestion phase of your RAG process as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fn import F # fn.py library\n",
    "\n",
    "data_loader = get_data_loader(\"file\")\n",
    "spitter = get_splitter(\"sentence_splitter\")\n",
    "embedder = get_embedder(\"sentence_embedder\")\n",
    "vector_db = get_vector_db(\"qdrant\", location=\":memory:\")\n",
    "\n",
    "data_loader.add_resource(dir=\"./data\")\n",
    "data = data_loader.load()\n",
    "\n",
    "result = F(data_loader.load) \n",
    "    >> F(splitter.split) \n",
    "    >> F(embedder.embed) \n",
    "    >> F(vector_db.upsert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to query your VectorDB, you can compose it like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who is the CEO of SpaceX?\"\n",
    "template = get_prompt_source(\"bodhiprompts\").find(\"extractive_qna\")\n",
    "\n",
    "answer = (\n",
    "    query\n",
    "    >> F(embedder.embed)\n",
    "    >> F(partial(vector_db.query, \"articles_collection\"))\n",
    "    >> F(lambda nodes: [node.text for node in nodes])\n",
    "    >> F(lambda nodes: {\"context\": \"\\n\\n\".join(nodes)})\n",
    "    >> F(partial(template.to_prompt, query = query))\n",
    "    >> F(llm.generate)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bodhi-guide",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "nbsphinx": {
   "execute": "never"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
