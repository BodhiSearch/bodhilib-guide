{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Composability (WIP)\n",
    "\n",
    "`bodhilib` library is designed with composability in mind. It takes many ideas from strict functional languages like `Haskell` to design and implement its interface.\n",
    "\n",
    "Using the bodhilib library, you can simplify the ingestion phase of your RAG process as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "1. Ensure bodhilib is installed, along with LLM plugin (`bodhiext.openai`), Embedder plugin (`bodhiext.sentence_transformers`), VectorDB plugin (`bodhiext.qdrant`).\n",
    "1. Ensure `fn.py` is installed for the functional composition methods.\n",
    "1. Ensure OPENAI_API_KEY is set in environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q bodhilib bodhiext.openai bodhiext.sentence_transformers bodhiext.qdrant fn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the components\n",
    "from bodhilib import get_llm, get_data_loader, get_embedder, get_vector_db, get_splitter\n",
    "\n",
    "data_loader = get_data_loader(\"file\")\n",
    "spitter = get_splitter(\"text_splitter\")\n",
    "embedder = get_embedder(\"sentence_transformers\")\n",
    "vector_db = get_vector_db(\"qdrant\", location=\":memory:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fn import Stream\n",
    "\n",
    "f = Stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fn import F # fn.py library\n",
    "\n",
    "data_loader.add_resource(dir=\"./data\")\n",
    "data = data_loader.load()\n",
    "\n",
    "result = F(data_loader.load) \n",
    "    >> F(splitter.split) \n",
    "    >> F(embedder.embed) \n",
    "    >> F(vector_db.upsert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to query your VectorDB, you can compose it like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who is the CEO of SpaceX?\"\n",
    "template = get_prompt_source(\"bodhiprompts\").find(\"extractive_qna\")\n",
    "\n",
    "answer = (\n",
    "    query\n",
    "    >> F(embedder.embed)\n",
    "    >> F(partial(vector_db.query, \"articles_collection\"))\n",
    "    >> F(lambda nodes: [node.text for node in nodes])\n",
    "    >> F(lambda nodes: {\"context\": \"\\n\\n\".join(nodes)})\n",
    "    >> F(partial(template.to_prompt, query = query))\n",
    "    >> F(llm.generate)\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bodhi-guide",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
