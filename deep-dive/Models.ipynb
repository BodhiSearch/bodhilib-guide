{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "The core models of bodhilib are -\n",
    "\n",
    "1. [Role](#role)\n",
    "1. [Source](#source)\n",
    "1. [Prompt](#prompt)\n",
    "1. [PromptStream](#promptstream)\n",
    "1. [PromptTemplate](#prompttemplate)\n",
    "1. [Document](#document)\n",
    "1. [Node](#node)\n",
    "\n",
    "Let's see the composition for each of these models.\n",
    "\n",
    "---\n",
    "\n",
    "## Role\n",
    "\n",
    "Role indicates the persona of the Prompt. The role has 3 possible values:\n",
    "\n",
    "1. system\n",
    "1. ai\n",
    "1. user\n",
    "\n",
    "### class Role\n",
    "\n",
    "```python\n",
    "class Role(str, Enum):\n",
    "    SYSTEM = \"system\"\n",
    "    AI = \"ai\"\n",
    "    USER = \"user\"\n",
    "```\n",
    "\n",
    "### Role = system\n",
    "The Role `system` indicates the inputs given to the LLM directly. These inputs can be used to control the output and ensure safe output is produced from the LLM, avoiding hallucination or Prompt injection attacks.\n",
    "\n",
    "### Role = ai\n",
    "The Role `ai` indicates the output generated by the LLM. During a chat conversation, the previous chat history with the LLM is passed as input back to the LLM for context. To indicate the output generated by LLM in the previous operation, role=ai is used.\n",
    "\n",
    "### Role = user\n",
    "The Role `user` is the input from the user to generate a response from LLM. The LLM generates the output corresponding to the input given by the user.\n",
    "\n",
    "---\n",
    "\n",
    "## Source\n",
    "\n",
    "The source indicates if a Prompt is provided as input, or generated as output from LLM.\n",
    "\n",
    "### class Source\n",
    "\n",
    "```python\n",
    "class Source(str, Enum):\n",
    "    INPUT = \"input\"\n",
    "    OUTPUT = \"output\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Prompt\n",
    "\n",
    "Prompt encapsulates the input to and output from the LLM. \n",
    "\n",
    "As an input, the **text** contains the query to the LLM, **role** is one of `system` or `user`, and **source** is `input`.\n",
    "\n",
    "As an output from the LLM, **text** contains the response from the LLM, **role** is `ai`, and **source** is `output`.\n",
    "\n",
    "### class Prompt\n",
    "\n",
    "```python\n",
    "class Prompt(BaseModel):\n",
    "    text: str\n",
    "    role: Role\n",
    "    source: Source\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## PromptStream\n",
    "\n",
    "The PromptStream allows asynchronous streaming of response from the LLM.\n",
    "\n",
    "```python\n",
    "class PromptStream(Iterator[Prompt]):\n",
    "    def __iter__(self) -> Iterator[Prompt]: ...\n",
    "    def __next__(self) -> Prompt: ...\n",
    "    @property\n",
    "    def text(self) -> str: ...\n",
    "```\n",
    "\n",
    "**PromptStream** uses Pythonic interface of a generator iterator to produce response as it is generated. So you can use `PromptStream` as:\n",
    "\n",
    "```python\n",
    "prompt_stream = llm.generate(...)\n",
    "for prompt in prompt_stream:\n",
    "    print(prompt.text, end=\"\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## PromptTemplate\n",
    "\n",
    "`PromptTemplate` allows you to generate prompt for your use-case injecting it with the right context. It re-uses the rich eco-system of python, and does not re-invent the wheel in the process. \n",
    "\n",
    "PromptTemplate supports 4 formats:\n",
    "\n",
    "1. `fstring`\n",
    "\n",
    "    For simple prompts involving variable injection, you can use the `fstring` format. It uses python's native f-string formatting and interpolation to inject your variables. You can then pass your variables to the `to_prompts` method to build your prompt.\n",
    "\n",
    "2. `jinja2`\n",
    "\n",
    "    For more complex prompts involving `loop`, `if-else` conditionals, you can uses `jinja2` templating library, and pass the template as a `jinja2` compatible template. You can then pass your variables to the `to_prompts` method build your prompt.\n",
    "\n",
    "3. `bodhilib-fstring`\n",
    "\n",
    "    `bodhilib-fstring` allows you to load simple prompts using `PromptSource` component. The prompts are serialized in bodhilib-prompt-template format, and uses `f-string` format for variable injections. Check out `PromptSource` component for details.\n",
    "\n",
    "4. `bodhilib-jinja2`\n",
    "\n",
    "    `bodhilib-jinja2` allows you to load complex prompts using `PromptSource` component. The prompts are serialized in bodhilib-prompt-template format, and uses `jinja2` templates for variable injections. Check out `PromptSource` component for details.\n",
    "\n",
    "### TemplateFormat\n",
    "\n",
    "```python\n",
    "TemplateFormat = Literal[\"fstring\", \"jinja2\", \"bodhilib-fstring\", \"bodhilib-jinja2\"]\n",
    "```\n",
    "\n",
    "### class PromptTemplate\n",
    "```python\n",
    "class PromptTemplate:\n",
    "    def __init__(\n",
    "        self,\n",
    "        template: str,\n",
    "        format: Optional[TemplateFormat] = \"fstring\",\n",
    "        metadata: Dict[str, Any] = Field(default_factory=dict),\n",
    "        vars: Dict[str, Any] = Field(default_factory=dict),\n",
    "    ) -> None: ...\n",
    "\n",
    "    def to_prompts(self, **kwargs: Dict[str, Any]) -> [Prompt]: ...\n",
    "```\n",
    "\n",
    "### PromptTemplate use-case\n",
    "```python\n",
    "# using prompt_template to generate dynamic prompt with variable injection\n",
    "prompt_template = PromptTemplate(template=..., format=\"jinja2\")\n",
    "prompts = prompt_template.to_prompts(var1=..., var2=...)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Document\n",
    "Document captures the resource loaded from a source. The main content is captured in field `text` of the Document, any other metadata like file location, url etc. are captured as part of `metadata` field.\n",
    "\n",
    "### class Document\n",
    "```python\n",
    "class Document(BaseModel):\n",
    "    text: str\n",
    "    metadata: Dict[str, Any]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Node\n",
    "`Node` captures the processible chunk for LLM operation. Document can be very large and cannot be processed in its original form. You can split Document in to processible entity Node. \n",
    "\n",
    "- The main content of the Node is captured in the field `text`\n",
    "- The parent resource from where this Node was split is captured in field `parent`\n",
    "- Any metadata related to Node is captured in field `metadata`\n",
    "- The embedding related to this Node is captured in field `embedding`\n",
    "- If the Node is persisted in a Vector DB, the record identifier is captured in field `id`\n",
    "\n",
    "```python\n",
    "class Node(BaseModel):\n",
    "    id: Optional[str]\n",
    "    text: str\n",
    "    parent: Optional[Document]\n",
    "    metadata: Dict[str, Any]\n",
    "    embedding: Optional[Embedding]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "ðŸŽ‰ We just got familiar with the models of bodhilib.\n",
    "\n",
    "Next, letâ€™s see different [Components](Components.ipynb) used in the library."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bodhi-guide",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "nbsphinx": {
   "execute": "never"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
